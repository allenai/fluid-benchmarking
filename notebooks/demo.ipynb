{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf74465",
   "metadata": {},
   "source": [
    "# Fluid Benchmarking Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f64ea4",
   "metadata": {},
   "source": [
    "The core entry point to Fluid Benchmarking is `evaluation.fluid_benchmarking()`. You need to provide:\n",
    "\n",
    "- `lm_responses_r`: an rpy2 vector of binary LM responses (0 = incorrect, 1 = correct), in the same item order as the IRT model.\n",
    "- `irt_model_r`: an rpy2 dataframe with the benchmark's IRT item parameters.\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "- `start_ability`: initial ability estimate.\n",
    "- `n_max`: maximum items to administer. \n",
    "- `selection_method`: method for item selection.\n",
    "- `estimation_method`: method for ability estimation.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- `abilities_fb`: list of provisional ability estimates after each administered item.\n",
    "- `items_fb`: list of selected item row indices in the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870ab3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rpy2.robjects as ro\n",
    "\n",
    "from fluid_benchmarking import config, datasets, evaluation, indexing, rutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83df0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose LM and benchmark\n",
    "lm = \"olmo2-7b\"\n",
    "benchmark = \"hellaswag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRT model for selected benchmark\n",
    "irt_model = datasets.load_irt_model(\n",
    "    config.HF_REPO_ID,\n",
    "    config.IRT_MODELS_PATH.format(benchmark)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33421df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results and filter to selected benchmark\n",
    "lm_eval_results = datasets.load_lm_eval_results(\n",
    "    config.HF_REPO_ID,\n",
    "    config.LM_EVAL_RESULTS_PATH.format(lm)\n",
    ")\n",
    "lm_eval_results_benchmark = indexing.filter_benchmark(\n",
    "    lm_eval_results, \n",
    "    benchmark\n",
    ")\n",
    "\n",
    "# Check that item order is identical to IRT model\n",
    "assert (lm_eval_results_benchmark.index == irt_model.index).all()\n",
    "\n",
    "# Pick checkpoint\n",
    "last_checkpoint = lm_eval_results_benchmark.columns[-1]\n",
    "lm_responses = np.array(lm_eval_results_benchmark[last_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83eec577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LM responses and IRT model to rpy2 objects\n",
    "lm_responses_r = ro.IntVector(lm_responses)\n",
    "irt_model_r = rutils.pandas2r(irt_model.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1731c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "start_ability = 0\n",
    "n_max = min(500, len(irt_model))\n",
    "selection_method = \"MFI\"  # Maximum Fisher information\n",
    "estimation_method = \"BM\"  # Bayes modal estimation (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf399cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Fluid Benchmarking\n",
    "abilities_fb, items_fb = evaluation.fluid_benchmarking(\n",
    "    lm_responses_r=lm_responses_r,\n",
    "    irt_model_r=irt_model_r,\n",
    "    start_ability=start_ability,\n",
    "    n_max=n_max,\n",
    "    selection_method=selection_method,\n",
    "    estimation_method=estimation_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32014aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 ability estimates: [0.404, 0.818, 1.035, 1.357, 1.587]\n",
      "First 5 administered items: ['hellaswag_760', 'hellaswag_3081', 'hellaswag_3518', 'hellaswag_8158', 'hellaswag_347']\n"
     ]
    }
   ],
   "source": [
    "# First k provisional ability estimates and administered items\n",
    "k = 5\n",
    "print(f\"First {k} ability estimates:\", [round(x, 3) for x in abilities_fb[:k]])\n",
    "print(f\"First {k} administered items:\", [irt_model.index[i] for i in items_fb[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5253af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2item = datasets.load_id_to_item_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f5993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example': 'Education and Communications: How to beat the pink tax. Read labels carefully. Before making a purchase, scrutinize the label. This can help you identify the pink tax.',\n",
       " 'choices': [\"You may think certain products are marked up fairly, because they contain different ingredients or come in greater amounts. However, you may find many women's products are similar to men's products but needlessly marked up.\",\n",
       "  'In particular, watch out for words and abbreviations which indicate a high quality product and rich background. Looking for this information can be a good indicator of whether or not the individual or business should buy this product.',\n",
       "  'Make sure it states in red letters exactly how many days the policies have expired.. Do not buy products or companies that claim to be pink tax-free.',\n",
       "  'This knowledge will help you be able to analyze the products you buy and plan those purchases accordingly. Here are some of the known labels that can help you locate pink tax : Lampshades summer coats winter coats rain gear clothes clothing. Realize the differences between these labels.']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect administered items\n",
    "id2item[irt_model.index[items_fb[0]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluid-benchmarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
